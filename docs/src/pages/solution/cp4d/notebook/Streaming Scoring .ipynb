{"cells": [{"metadata": {"collapsed": true, "id": "77627b20-47a4-478e-ba21-b9bb2ad03938"}, "cell_type": "markdown", "source": "# Install Confluent Kafka Library"}, {"metadata": {"id": "81ab3b46295c4c26843b510ed4397530"}, "cell_type": "code", "source": "!pip install confluent-kafka", "execution_count": 1, "outputs": [{"name": "stdout", "text": "Collecting confluent-kafka\n  Downloading confluent_kafka-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (8.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.1 MB 18.2 MB/s eta 0:00:01\ufffd\ufffd        | 6.1 MB 18.2 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: confluent-kafka\nSuccessfully installed confluent-kafka-1.5.0\n", "output_type": "stream"}]}, {"metadata": {"id": "60352d7124a54f52865d65112f3b2bbf"}, "cell_type": "markdown", "source": "# Set Environment Variables for Kafka Application"}, {"metadata": {"id": "7729cf5cffc048b28e5c9515206c5d13"}, "cell_type": "code", "source": "# @hidden_cell\n%env KAFKA_BROKERS=eda-dev-kafka-bootstrap-eventstreams.gse-eda-2021-1-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\n%env KAFKA_USER=\n%env KAFKA_PASSWORD=\n%env KAFKA_CERT=/project_data/data_asset/es-cert-new.pem", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "env: KAFKA_BROKERS=eda-dev-kafka-bootstrap-eventstreams.gse-eda-2021-1-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\nenv: KAFKA_USER=\nenv: KAFKA_PASSWORD=\nenv: KAFKA_CERT=/project_data/data_asset/es-cert-new.pem\n", "name": "stdout"}]}, {"metadata": {"id": "13403b9724da45678f1587509e69f40c"}, "cell_type": "markdown", "source": "# Define Kafka Producer Class"}, {"metadata": {"id": "3bd96ccb85bb44c3b646a3d5e3172786"}, "cell_type": "code", "source": "import json, os\nfrom confluent_kafka import KafkaError, Producer\n\nclass KafkaProducer:\n\n    def __init__(self, groupID = \"KafkaProducer\"):\n        # Get the producer configuration\n        self.producer_conf = self.getProducerConfiguration(groupID)\n        # Create the producer\n        self.producer = Producer(self.producer_conf)\n\n    def getProducerConfiguration(self,groupID):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n            \n            # Print out the producer configuration\n            self.printProducerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaProducer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n\n    def printProducerConfiguration(self,options):\n        # Printing out producer config for debugging purposes        \n        print(\"[KafkaProducer] - This is the configuration for the producer:\")\n        print(\"[KafkaProducer] - -------------------------------------------\")\n        print('[KafkaProducer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaProducer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaProducer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaProducer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaProducer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaProducer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print(\"[KafkaProducer] - -------------------------------------------\")\n\n    def delivery_report(self,err, msg):\n        \"\"\" Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). \"\"\"\n        if err is not None:\n            print('[KafkaProducer] - [ERROR] - Message delivery failed: {}'.format(err))\n        else:\n            print('[KafkaProducer] - Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n\n    def publishEvent(self, topicName, eventToSend, keyName):\n        # Print the event to send\n        dataStr = json.dumps(eventToSend)\n        # Produce the message\n        self.producer.produce(topicName,key=eventToSend[keyName],value=dataStr.encode('utf-8'), callback=self.delivery_report)\n        # Flush\n        self.producer.flush()\n", "execution_count": 3, "outputs": []}, {"metadata": {"id": "0bacffe4f918433d8b0271c9dc7259a5"}, "cell_type": "markdown", "source": "# Define Kafka Consumer Class"}, {"metadata": {"id": "383f18f468b3453285f9ce5cd958f4b1"}, "cell_type": "code", "source": "import json,os,csv\nfrom confluent_kafka import Consumer, KafkaError\n\n\nclass KafkaConsumer:\n\n    def __init__(self, topic_name = \"kafka-producer\", groupID = 'KafkaConsumer-F21', autocommit = True):\n        # Get the consumer configuration\n        self.consumer_conf = self.getConsumerConfiguration(groupID, autocommit)\n        # Create the Avro consumer\n        self.consumer = Consumer(self.consumer_conf)\n        # Subscribe to the topic\n        self.consumer.subscribe([topic_name])\n\n    def getConsumerConfiguration(self, groupID, autocommit):\n        try:\n            options ={\n                    'bootstrap.servers': os.environ['KAFKA_BROKERS'],\n                    'group.id': groupID,\n                    'auto.offset.reset': \"earliest\",\n                    'enable.auto.commit': autocommit,\n            }\n            if (os.getenv('KAFKA_PASSWORD','') != ''):\n                # Set security protocol common to ES on prem and on IBM Cloud\n                options['security.protocol'] = 'SASL_SSL'\n                # Depending on the Kafka User, we will know whether we are talking to ES on prem or on IBM Cloud\n                # If we are connecting to ES on IBM Cloud, the SASL mechanism is plain\n                if (os.getenv('KAFKA_USER','') == 'token'):\n                    options['sasl.mechanisms'] = 'PLAIN'\n                # If we are connecting to ES on OCP, the SASL mechanism is scram-sha-512\n                else:\n                    options['sasl.mechanisms'] = 'SCRAM-SHA-512'\n                # Set the SASL username and password\n                options['sasl.username'] = os.getenv('KAFKA_USER','')\n                options['sasl.password'] = os.getenv('KAFKA_PASSWORD','')\n            # If we are talking to ES on prem, it uses an SSL self-signed certificate.\n            # Therefore, we need the CA public certificate for the SSL connection to happen.\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))):\n                options['ssl.ca.location'] = os.getenv('KAFKA_CERT','/certs/es-cert.pem')\n\n            # Print out the producer configuration\n            self.printConsumerConfiguration(options)\n\n            return options\n\n        except KeyError as error:\n            print('[KafkaConsumer] - [ERROR] - A required environment variable does not exist: ' + error)\n            exit(1)\n    \n    def printConsumerConfiguration(self,options):\n        # Printing out consumer config for debugging purposes        \n        print(\"[KafkaConsumer] - This is the configuration for the consumer:\")\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n        print('[KafkaConsumer] - Bootstrap Server:      {}'.format(options['bootstrap.servers']))\n        if (os.getenv('KAFKA_PASSWORD','') != ''):\n            # Obfuscate password\n            if (len(options['sasl.password']) > 3):\n                obfuscated_password = options['sasl.password'][0] + \"*****\" + options['sasl.password'][len(options['sasl.password'])-1]\n            else:\n                obfuscated_password = \"*******\"\n            print('[KafkaConsumer] - Security Protocol:     {}'.format(options['security.protocol']))\n            print('[KafkaConsumer] - SASL Mechanism:        {}'.format(options['sasl.mechanisms']))\n            print('[KafkaConsumer] - SASL Username:         {}'.format(options['sasl.username']))\n            print('[KafkaConsumer] - SASL Password:         {}'.format(obfuscated_password))\n            if (os.path.isfile(os.getenv('KAFKA_CERT','/certs/es-cert.pem'))): \n                print('[KafkaConsumer] - SSL CA Location:       {}'.format(options['ssl.ca.location']))\n        print('[KafkaConsumer] - Offset Reset:          {}'.format(options['auto.offset.reset']))\n        print('[KafkaConsumer] - Autocommit:            {}'.format(options['enable.auto.commit']))\n        print(\"[KafkaConsumer] - -------------------------------------------\")\n    \n    # Prints out and returns the decoded events received by the consumer\n    def traceResponse(self, msg):\n        print('[KafkaConsumer] - Next Message consumed from {} partition: [{}] at offset: {}\\n\\tkey: {}\\n\\tvalue: {}'\n                    .format(msg.topic(), msg.partition(), msg.offset(), msg.key().decode('utf-8'), msg.value().decode('utf-8')))\n\n    # Polls for next event and score deployed model\n    def pollNextEvent(self):\n\n      # Poll for messages\n      msg = self.consumer.poll(timeout=10.0)\n\n            \n      if msg is None:\n        print(\"[KafkaConsumer] - [INFO] - No new messages on the topic\")\n        \n      while not (msg is None):\n\n        if msg.error():\n            if (\"PARTITION_EOF\" in msg.error()):\n                print(\"[KafkaConsumer] - [INFO] - End of partition\")\n            else:\n                print(\"[KafkaConsumer] - [ERROR] - Consumer error: {}\".format(msg.error()))\n        else:\n            \n            # Read Telemetry data from Kafka topic\n            loaded_json = json.loads(msg.value().decode('utf-8'))\n            payload = loaded_json[\"payload\"].strip('()')\n            values = list(payload.split(\",\")) \n            \n            fields = ['container_id', 'timestamp', 'product_id', 'temperature','target_temperature','ambiant_temperature','kilowatts','content_type','oxygen_level','nitrogen_level', \\\n                'carbon_dioxide_level', 'humidity_level', 'fan_1', 'vent_2', 'vent_3', 'time_door_open', 'latitude', 'longitude', 'defrost_cycle', 'maintenance_required'] \n            \n            # Define fields needed for model\n            fields_model= [\"temperature\",\"ambiant_temperature\",\"kilowatts\",\"oxygen_level\",\"nitrogen_level\",\"humidity_level\",\"fan_1\",\"vent_2\"]\n    \n            col_idx_list = []\n            for i in fields_model:\n                col_idx=fields.index(i)\n                col_idx_list.append(col_idx)\n    \n            values_model = [float(values[i]) for i in col_idx_list]\n    \n            payload_scoring = {\"input_data\": [{\"fields\": fields_model,\"values\": [values_model]}]}\n            \n            # Call anomaly detection model\n            scoring_response = wml_client.deployments.score(deployment_id, payload_scoring)\n        \n            prediction_label = scoring_response['predictions'][0]['values'][0][0]\n            \n            # Send anomalies only to Kafka Topic\n            if prediction_label == 1:\n                event = {\"eventKey\" : \"Anomaly Detection Topic\", \"message\" : scoring_response}         \n                kafka_producer = KafkaProducer() \n                kafka_producer.publishEvent(\"Anomaly Detection Topic\",event,\"eventKey\")\n        \n            \n            \n        msg = self.consumer.poll(timeout=10.0)    \n      \n      f.close()\n      \n      \n    \n    def close(self):\n        self.consumer.close()", "execution_count": 4, "outputs": []}, {"metadata": {"id": "7b18bfc573784553869e4207ca71da7a"}, "cell_type": "markdown", "source": "# Authenticate to WML Client"}, {"metadata": {"id": "094f798df51b4340876db423f8b296f1"}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\ntoken = os.environ['USER_ACCESS_TOKEN']\nwml_credentials = {\n                   \"token\": token,\n                   \"instance_id\" : \"wml_local\",\n                   \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n                   \"version\": \"3.5\"\n}\nwml_client = APIClient(wml_credentials)\n\n# Obtain the UId of your space\ndef guid_from_space_name(client, space_name):\n    instance_details = client.service_instance.get_details()\n    space = client.spaces.get_details()\n    return(next(item for item in space['resources'] if item['entity'][\"name\"] == space_name)['metadata']['id'])\n\nspace_uid = guid_from_space_name(wml_client, 'Vaccine Model Production Deployment Space')\nprint(\"Space UID = \" + space_uid)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Space UID = 285c322f-c23f-4784-83fa-bf5dd73d06aa\n", "name": "stdout"}]}, {"metadata": {"id": "a329fda5a4154a719b5dccc8cf31f78b"}, "cell_type": "code", "source": "wml_client.set.default_space(\"b409e49d-b3d7-4436-81d1-3172c821f920\")", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}]}, {"metadata": {"id": "87b853313c284ec9b4240653ae4572d6"}, "cell_type": "code", "source": "deployment_id = '85bb8780-736c-4c07-890c-672ca0495307' ", "execution_count": 18, "outputs": []}, {"metadata": {"id": "c6c80632f63c4f8a872304d7c1e142bf"}, "cell_type": "markdown", "source": "# Read Events from Kafka Topic, Call Model Endpoint, Return Anomalous Events Back to Topic"}, {"metadata": {"id": "34beb5438a8046d894270a5006667327"}, "cell_type": "code", "source": "\nif __name__ == '__main__':\n    \n    # Create a Kafka Consumer\n    kafka_consumer = KafkaConsumer(\"telemetries\")\n    # Poll for next message\n    message = kafka_consumer.pollNextEvent()\n    # Close the consumer\n    kafka_consumer.close()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "[KafkaConsumer] - This is the configuration for the consumer:\n[KafkaConsumer] - -------------------------------------------\n[KafkaConsumer] - Bootstrap Server:      eda-dev-kafka-bootstrap-eventstreams.gse-eda-2021-1-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\n[KafkaConsumer] - Security Protocol:     SASL_SSL\n[KafkaConsumer] - SASL Mechanism:        SCRAM-SHA-512\n[KafkaConsumer] - SASL Username:         qijun-test\n[KafkaConsumer] - SASL Password:         b*****l\n[KafkaConsumer] - SSL CA Location:       /project_data/data_asset/es-cert-new.pem\n[KafkaConsumer] - Offset Reset:          earliest\n[KafkaConsumer] - Autocommit:            True\n[KafkaConsumer] - -------------------------------------------\n[KafkaProducer] - This is the configuration for the producer:\n[KafkaProducer] - -------------------------------------------\n[KafkaProducer] - Bootstrap Server:      eda-dev-kafka-bootstrap-eventstreams.gse-eda-2021-1-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\n[KafkaProducer] - Security Protocol:     SASL_SSL\n[KafkaProducer] - SASL Mechanism:        SCRAM-SHA-512\n[KafkaProducer] - SASL Username:         qijun-test\n[KafkaProducer] - SASL Password:         b*****l\n[KafkaProducer] - SSL CA Location:       /project_data/data_asset/es-cert-new.pem\n[KafkaProducer] - -------------------------------------------\n0\n", "name": "stdout"}]}, {"metadata": {"id": "9fb7203333d841f9801297b0ce0f69bb"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}