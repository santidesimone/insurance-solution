---
title: Order management and optimization deployed with EventStreams
description: Order management and optimization demonstration
--- 

1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:

   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.
   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).
   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).
   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.


## Deploy Postgresql


```shell
git clone https://github.com/ibm-cloud-architecture/vaccine-gitops.git
cd vaccine-gitops/environments
oc apply -k 
```

## Deploy the Vaccine Order Service with Event Streams

This microservice is built using maven and Quarkus extensions. In this current main project we have 
We have already pushed the last version of this service on dockerhub, if you do not want to build it. 

1. Ensure you are working inside the correct project via the following `oc` command:

  ```shell
  export PROJECT_NAME=vaccine-solution
  oc project ${PROJECT_NAME}
  ```

1. Export the value of your Event Streams cluster name into an environment variable:

  ```shell
  export CLUSTER_NAME=eda-dev
  export EVENTSTREAMS_NS=eventstreams
  ```
   * To check what the name of your Event Streams cluster name is do:
   ```shell
   $ oc get eventstreams -n ${EVENTSTREAMS_NS}
   NAME           STATUS
   eda-dev    Ready
   ```

1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:

  ```shell
   oc apply -f event-streams/infrastructure/service-account.yaml
   oc adm policy add-scc-to-user anyuid -z vaccine-runtime -n vaccine-solution
   oc apply -k event-streams/infrastructure/postgres

  ```

1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:

  ```shell
  oc get kafkausers -n $EVENTSTREAMS_NS 
  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION
  # app-scram                           eda-dev   scram-sha-512    simple
  # app-tls                             eda-dev   tls              simple
  export KAFKA_USER=app-tls
  ```
1. Copy Kafka TLS user certificate to target project:

  ```shell
  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace="'${PROJECT_NAME}'"' | oc apply -f -
  ```
1. Get Kafka bootstrap server URL within

   ```shell
   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath="{.status.ingress[0].host}:443")
   ```

 1. Copy TLS server CA certificate from eventstreams project to local project with the command:

   ```shell
   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name="kafka-cluster-ca-cert"' |jq -r '.metadata.namespace="'${PROJECT_NAME}'"' | oc apply -f -
   ``` 
1. Deploy the order management microservice

  ```shell
  oc apply -f ./apps/order-mgt/base/order-mgt-configmap.yaml
  oc apply -f ./apps/order-mgt/base/order-mgt-deployconfig.yaml
  ```

### Deploy Debezium CDC connector

The [Event Streams product documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) goes over the tasks to be done to config Kafka Connect cluster, but we can summarize them for our use case as:

* Start a Kafka connector cluster: We use the custom resource definition called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called `KafkaConnector`. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done (See also [this note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/#set-up-the-kafka-connect-cluster) for the same type of setting). 
  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.
  * Update the following values in this file: `bootstrapServers` and `secretName: tls`  to `secretName: <yourTLSuser>` and the Server ca certificate secretName like `kafka-cluster-ca-cert`.
   ```yaml
   tls:
    trustedCertificates:
      - certificate: ca.crt
        secretName: kafka-cluster-ca-cert
   ```
  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n ${EVENTSTREAMS_NS}`
  * Validate it via: 

   ```shell
   oc get kafkaconnects2i -n ${EVENTSTREAMS_NS}
   oc describe kafkaconnects2i connect-cluster -n ${EVENTSTREAMS_NS}
   ```
* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. This step was already done and the debezium connector jars are in the [environment/cdc/my-plugins/debezium-connector)](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/tree/main/environment/cdc/my-plugins/debezium-connector)

  ```
  ├── my-plugins
  │   └── debezium-connector
  │       ├── CHANGELOG.md
  │       ├── CONTRIBUTE.md
  │       ├── COPYRIGHT.txt
  │       ├── LICENSE-3rd-PARTIES.txt
  │       ├── LICENSE.txt
  │       ├── README.md
  │       ├── README_ZH.md
  │       ├── debezium-api-1.4.0.Final.jar
  │       ├── debezium-connector-postgres-1.4.0.Final.jar
  │       ├── debezium-core-1.4.0.Final.jar
  │       ├── failureaccess-1.0.1.jar
  │       ├── guava-30.0-jre.jar
  │       ├── postgresql-42.2.14.jar
  │       └── protobuf-java-3.8.0.jar
  └── pg-connector.yaml
  ```

* Deploy the connector configuration:

  ```shell
  # pwd = .../environment/cdc/
  oc start-build connect-cluster-connect --from-dir ./my-plugins/ --follow -n ${EVENTSTREAMS_NS}
  #...
  # Storing signatures
  # Successfully pushed image-registry.openshift-image-registry.svc:5000/eventstreams/connect-cluster-connect@sha256:9315b6a6c8f904d0fb5a57f67ba4c9067c6c8264814f283151b20b9d6f147092
  # Push successful
  ```
* Modify the `pg-connector.yaml` from the `environment/cdc` folder to configure the Postgres datasource credentials:
  ```yaml
  config: 
    database.dbname: postgres
    database.hostname: postgres.vaccineorder.svc
    database.password: pgpwd
    database.port: 5432
    database.server.name: vaccine
    database.user: postgres
    table.whitelist: public.orderevents
    plugin.name: pgoutput
  ```
* Then start the connector: `oc apply -f pg-connector.yaml -n ${EVENTSTREAMS_NS}`
* Verify it is running: `oc describe kafkaconnector pg-connector -n ${EVENTSTREAMS_NS}`, you should see one task running. 
* Looking at the pod trace for the connector you should see a successful connection, something like:

```
Successfully tested connection for jdbc:postgresql://postgres.vaccineorder.svc:5432/postgres with user 'postgres' 
```
* A new topic may have been created with the name of the table replicated: `vaccine.public.orderevents` with new messages mapping the rows in the table.